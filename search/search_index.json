{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to intracranial_ephys_utils","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#about","title":"About","text":"<p>This project serves to be a starting point for sEEG, iEEG, and microwire LFP analyses in human neuroscience datasets. Importantly, this projects builds heavily off of neo to do I/O, and ephyviewer to manually annotate or curate event times, such as epileptiform activity, or task relevant variables.</p> <p>If there's anything you'd like to see this code do, please add an issue on github, or email me directly about it at</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"documentation/load_data/","title":"Loading data","text":"<p>Here is some sample documentation. Let's start with the loading data script.</p>"},{"location":"documentation/load_data/#data-loading-functions","title":"Data Loading functions","text":""},{"location":"documentation/load_data/#intracranial_ephys_utils.load_data.get_file_info","title":"<code>get_file_info(directory, start, file_extension)</code>","text":"<p>Look in directory for files that start and end with something. Raise error if more than one match is found Useful for neuralynx files where they may be multiple session, and file identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to the directory we want to look through.</p> required <code>start</code> <code>string</code> <p>start prefix of desired file</p> required <code>file_extension</code> <code>string</code> <p>end suffix of desired file</p> required <p>Returns:</p> Name Type Description <code>File_path</code> <code>Path</code> <p>first file path that matches</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_file_info(Path(os.getcwd()), 'L', '.txt') # grab the license from this repo\nPosixPath('/home/runner/work/intracranial_ephys_utils/intracranial_ephys_utils/LICENSE.txt')\n</code></pre> Source code in <code>intracranial_ephys_utils/load_data.py</code> <pre><code>def get_file_info(directory: Path, start: str, file_extension: str) -&gt; Path:\n    \"\"\"\n    Look in directory for files that start and end with something. Raise error if more than one match is found\n    Useful for neuralynx files where they may be multiple session, and file identifiers.\n\n    Args:\n        directory (Path):  Path to the directory we want to look through.\n        start (string):  start prefix of desired file\n        file_extension (string):  end suffix of desired file\n\n    Returns:\n        File_path: first file path that matches\n\n    Examples:\n        &gt;&gt;&gt; get_file_info(Path(os.getcwd()), 'L', '.txt') # grab the license from this repo\n        PosixPath('/home/runner/work/intracranial_ephys_utils/intracranial_ephys_utils/LICENSE.txt')\n    \"\"\"\n    files_list = os.listdir(directory)\n    files = [file_path for file_path in files_list if file_path.endswith(file_extension) and\n             file_path.startswith(start)]\n\n    if len(files) &gt; 1:\n        print(f'Found files: {files}')\n        raise ValueError(f\"More than one file matching {start}*{file_extension}, consider a more specific function call.\")\n    file_path = directory / files[0]\n    return file_path\n</code></pre>"},{"location":"documentation/load_data/#intracranial_ephys_utils.load_data.read_file","title":"<code>read_file(file_path)</code>","text":"<p>Lazy reader of specific Neuralynx files. Will probably be removed eventually</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Absolute path to a neuralynx .ncs file.</p> required <p>Returns:</p> Name Type Description <code>reader_object</code> <code>NeuralynxRawIO</code> <p>neo reader object for neuralynx data</p> Source code in <code>intracranial_ephys_utils/load_data.py</code> <pre><code>def read_file(file_path: Path) -&gt; NeuralynxRawIO:\n    \"\"\"\n    Lazy reader of specific Neuralynx files. Will probably be removed eventually\n\n    Args:\n        file_path (Path):  Absolute path to a neuralynx .ncs file.\n\n    Returns:\n        reader_object: neo reader object for neuralynx data\n    \"\"\"\n    stems = os.path.split(file_path)\n    reader = NeuralynxRawIO(dirname=stems[0], include_filenames=stems[1])\n    return reader\n</code></pre>"},{"location":"documentation/load_data/#intracranial_ephys_utils.load_data.get_event_times","title":"<code>get_event_times(neuralynx_data_directory, extension=None)</code>","text":"<p>Looks at just the events file for a Neuralynx data directory to get timestamps(default is seconds) and labels for recording events. This is useful if researcher of clinician manually annotated events while recording. Keep in mind however, that the events file is sometimes not in accordance with the data itself, due to a referencing issue.</p> <p>Parameters:</p> Name Type Description Default <code>neuralynx_data_directory</code> <code>Path</code> <p>Directory of Neuralynx data, that stores at least one .nev file</p> required <code>extension</code> <code>Optional[str]</code> <p>optional(default None). If there are multiple .nev file, can specify the desired .nev by the end of the filename</p> <code>None</code> <p>Returns:</p> Name Type Description <code>event_times</code> <code>list</code> <p>machine time in microseconds. To convert this to seconds relative to recording start, convert to seconds and subtract from global_start</p> <code>event_labels</code> <code>list</code> <p>Whatever the annotation was for each event.</p> <code>global_start</code> <code>float</code> <p>Machine code time beginning of recording(seconds)</p> <code>event_file</code> <code>str</code> <p>File name for events file (useful in case there are two events files)</p> Source code in <code>intracranial_ephys_utils/load_data.py</code> <pre><code>def get_event_times(neuralynx_data_directory: Path, extension: Optional[str] = None) -&gt; tuple[list, list, float, str]:\n    \"\"\"\n    Looks at just the events file for a Neuralynx data directory to get timestamps(default is seconds) and labels\n    for recording events. This is useful if researcher of clinician manually annotated events while recording. Keep in\n    mind however, that the events file is sometimes not in accordance with the data itself, due to a referencing issue.\n\n    Args:\n        neuralynx_data_directory (Path): Directory of Neuralynx data, that stores at least one .nev file\n        extension: optional(default None). If there are multiple .nev file, can specify the desired .nev by the end of the filename\n\n    Returns:\n        event_times : machine time in microseconds. To convert this to seconds relative to recording start, convert to seconds and subtract from global_start\n        event_labels : Whatever the annotation was for each event.\n        global_start : Machine code time beginning of recording(seconds)\n        event_file : File name for events file (useful in case there are two events files)\n    \"\"\"\n    # Obtained in seconds and assumes that the start of the file (not necessarily the task) is 0.\n\n    all_files = os.listdir(neuralynx_data_directory)\n\n    # Find the events file, note just the filename, not the full path\n    if extension is None:\n        events_files = [file_path for file_path in all_files if file_path.startswith('Events') or file_path.endswith('.nev')]\n        # full_events_filepath = get_file_info(neuralynx_data_directory, 'Events', '.nev')\n        # events_file = full_events_filepath.name\n    else:\n        events_files = [file_path for file_path in all_files if (file_path.startswith('Events') and\n                                                                file_path.endswith(extension))]\n        # full_events_filepath = get_file_info(neuralynx_data_directory, 'Events', extension)\n        # events_file = full_events_filepath.name\n\n    if len(events_files) == 1:\n        # with one events file, read in the photodiode file\n        events_file = events_files[0]\n        event_reader = read_file(neuralynx_data_directory / events_file)\n        event_reader.parse_header()\n\n        ph_file = events_file.replace(\".nev\", \".ncs\")\n        ph_file = ph_file.replace(\"Events\", \"photo1\")\n        if os.path.exists(neuralynx_data_directory / ph_file):\n            ph_reader = read_file(neuralynx_data_directory / ph_file)\n        else:\n            # pick other file by random\n            file_list = os.listdir(neuralynx_data_directory)\n            better_file_list = [file for file in file_list if file.endswith('.ncs')]\n            ph_reader = read_file(neuralynx_data_directory / better_file_list[0])\n            warnings.warn(f\"No photodiode file found. Using {better_file_list[0]}\")\n        try:\n            ph_reader.parse_header()\n            global_start = ph_reader.global_t_start\n        except OSError:\n            warnings.warn(\"OSError. Inquire further. Previous errors were from a header missing information.\")\n        # global_start_event_reader = event_reader.global_t_start\n        try:\n            event_times, _, event_labels = event_reader.get_event_timestamps()\n            if 'global_start' not in locals():\n                global_start = event_times[0]\n                warnings.warn(\"No files to choose from. Is there neuralynx data in this folder?\")\n        except IndexError:\n            warnings.warn(\"No events found\")\n            event_times, event_labels = [], []\n            ph_path = get_file_info(neuralynx_data_directory, \"photo\", \".ncs\")\n            ph_reader = read_file(ph_path)\n            ph_reader.parse_header()\n            global_start = ph_reader.global_t_start\n    elif len(events_files) == 0:\n        warnings.warn(\"No events file found, Using Photodiode file to get global machine time start\")\n        event_times, event_labels = [], []\n        ph_path = get_file_info(neuralynx_data_directory, \"photo\", \".ncs\")\n        ph_reader = read_file(ph_path)\n        ph_reader.parse_header()\n        global_start = ph_reader.global_t_start\n        events_file = None\n    else:\n        print(f'Found files: {events_files}')\n        raise ValueError(f\"More than one file matching {events_files}, consider a more specific function call.\")\n    return event_times, event_labels, global_start, events_file\n</code></pre>"},{"location":"documentation/load_data/#intracranial_ephys_utils.load_data.missing_samples_check","title":"<code>missing_samples_check(file_path)</code>","text":"<p>This script checks for missing samples in neuralynx files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path object where data file is.</p> required <p>Returns:</p> Name Type Description <code>skipped_samples</code> <code>list</code> <p>integer number of skipped samples</p> <code>t_starts</code> <code>list</code> <p>The start times for each segment</p> <code>seg_sizes</code> <code>list</code> <p>The lengths of each Neo segment</p> Source code in <code>intracranial_ephys_utils/load_data.py</code> <pre><code>def missing_samples_check(file_path: Path) -&gt; tuple[list, list, list]:\n    \"\"\"\n    This script checks for missing samples in neuralynx files.\n\n    Args:\n        file_path: Path object where data file is.\n\n    Returns:\n        skipped_samples: integer number of skipped samples\n        t_starts: The start times for each segment\n        seg_sizes: The lengths of each Neo segment\n    \"\"\"\n    file_reader = read_file(file_path)\n    file_reader.parse_header()\n    n_segments = file_reader._nb_segment\n    t_starts = []\n    seg_sizes = []\n    sampling_rate = file_reader.get_signal_sampling_rate()\n    skipped_samples = []\n    diffs = []\n    # Start of the last segment + size of last segment - start of the task according to event\n    # ph_signal_estimate = file_reader.get_signal_t_start(block_index=0, seg_index=n_segments - 1) \\\n    #                      * sampling_rate + \\\n    #                      float(file_reader.get_signal_size(block_index=0, seg_index=n_segments - 1))\n    # ph_signal = np.zeros((int(ph_signal_estimate), 1))\n    for i in range(n_segments):\n        t_start = file_reader.get_signal_t_start(block_index=0, seg_index=i)\n        seg_size = file_reader.get_signal_size(block_index=0, seg_index=i)\n        # ph_signal_segment = file_reader.get_analogsignal_chunk(seg_index=i)\n        # start_index = int(t_start * sampling_rate)\n        # ph_signal[start_index:start_index + seg_size] = ph_signal_segment\n        if i &gt; 0:\n            # This part of the script looks for missing samples\n            t_end = float(seg_sizes[i - 1] / sampling_rate) + t_starts[- 1]\n            diff = abs(t_start - t_end) * sampling_rate\n            skipped_samples.append(round(diff))\n            diffs.append(abs(t_start - t_end))\n        t_starts.append(t_start)\n        seg_sizes.append(seg_size)\n        print(diffs)\n    return skipped_samples, t_starts, seg_sizes\n</code></pre>"},{"location":"documentation/load_data/#intracranial_ephys_utils.load_data.read_task_ncs","title":"<code>read_task_ncs(folder_name, file, task=None, events_file=None, interp_type='linear')</code>","text":"<p>Read neuralynx data into an array, with sampling rate, and start time of the task. To deal with discontinuities and dropped samples, we take a pragmatic approach. We assume continuous sampling, and if there are inconsistencies between the number of samples in segments and the array itself, we fill in samples by interpolating. Ideally this spits out neuralynx data in the form of an array, with the sampling rate, and the start time of the task</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>Path</code> <p>Path object that tells the path of the structure</p> required <code>file</code> <code>str</code> <p>filename we want to read currently</p> required <code>task</code> <code>Optional[str]</code> <p>(string, optional) that matches the event label in the actual events file. Ideally it matches the name of the task</p> <code>None</code> <code>events_file</code> <code>Optional[Path]</code> <p>(path, optional) needed if task argument is provided, this used to be the .nev file, but I found it useless so now it's a csv file that I generate via the scripts in manual_process</p> <code>None</code> <code>interp_type</code> <code>Optional[str]</code> <p>(string, optional) what type of interpolation to do in the case of missing data, choices are</p> <code>'linear'</code> <p>linear or cubic, default is linear</p> <p>Returns:</p> Name Type Description <code>ncs_signal</code> <code>array</code> <p>ndarray - signal in the file in np array format</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate for signal</p> <code>interp</code> <code>array</code> <p>ndarray - same size as ncs_signal and timestamps, tells you whether data was interpolated in that point, useful if finding weird things in data</p> <code>timestamps</code> <code>array</code> <p>an array that gives the timestamps from the ncs file using the start and stop task segments,</p> <p>this is in seconds, from the start of the .ncs file recording</p> Source code in <code>intracranial_ephys_utils/load_data.py</code> <pre><code>def read_task_ncs(folder_name: Path, file: str, task: Optional[str]=None, events_file: Optional[Path]=None,\n                  interp_type: Optional[str]='linear') -&gt; tuple[np.array, float, np.array, np.array]:\n    \"\"\"\n    Read neuralynx data into an array, with sampling rate, and start time of the task.\n    To deal with discontinuities and dropped samples, we take a pragmatic approach. We assume continuous sampling, and\n    if there are inconsistencies between the number of samples in segments and the array itself, we fill in samples by\n    interpolating.\n    Ideally this spits out neuralynx data in the form of an array, with the sampling rate, and the start time of the task\n\n    Args:\n        folder_name: Path object that tells the path of the structure\n        file: filename we want to read currently\n        task: (string, optional) that matches the event label in the actual events file. Ideally it matches the name of the task\n        events_file: (path, optional) needed if task argument is provided, this used to be the .nev file, but I found it useless so now it's a csv file that I generate via the scripts in manual_process\n        interp_type: (string, optional) what type of interpolation to do in the case of missing data, choices are\n    linear or cubic, default is linear\n\n    Returns:\n        ncs_signal: ndarray - signal in the file in np array format\n        sampling_rate: sampling rate for signal\n        interp: ndarray - same size as ncs_signal and timestamps, tells you whether data was interpolated in that point, useful if finding weird things in data\n        timestamps: an array that gives the timestamps from the ncs file using the start and stop task segments,\n    this is in seconds, from the start of the .ncs file recording\n    \"\"\"\n\n    file_path = folder_name / file\n    ncs_reader = read_file(file_path)\n    ncs_reader.parse_header()\n    n_segments = ncs_reader._nb_segment\n    sampling_rate = ncs_reader.get_signal_sampling_rate()\n\n    # This loop is to get around files that have weird events files, or task wasn't in the annotation\n    if task is not None:\n        if events_file is None:\n            raise ValueError(\"Need to provide event file\")\n        labels_file = pd.read_csv(events_file)\n        task_label = labels_file[labels_file.label == f\"{task} duration\"]\n        task_start = task_label['time'].iloc[0].astype(float)  # seconds from start of file\n        task_end = task_start + task_label['duration'].iloc[0].astype(float)\n\n        # print(float(ncs_reader.get_signal_size(block_index=0, seg_index=n_segments - 1))/sampling_rate)\n\n        # The following block looks for the time of the start and end of the task we care about\n        task_start_segment_index = None\n        task_end_segment_index = None\n        task_start_search = True\n        for i in range(n_segments):\n            time_segment_start = ncs_reader.get_signal_t_start(block_index=0, seg_index=i)\n            if time_segment_start &lt; task_start:\n                continue\n            elif (time_segment_start &gt;= task_start) and (time_segment_start &lt; task_end):\n                # The first time this is run, the task_start_search bit flips\n                if task_start_search:\n                    task_start_search = False\n                    # We take the index before because time_segment_start may not overlap with the start of the segment\n                    # and this is looking from below, so overlap is with previous segment\n                    task_start_segment_index = max(i - 1, 0)\n            elif time_segment_start &gt; task_end:\n                # The end isn't as important if we overshoot\n                task_end_segment_index = i-1\n                break\n            if i == n_segments-1:\n                task_end_segment_index = n_segments-1\n    else:\n        task_start_segment_index = 0\n        task_end_segment_index = n_segments-1\n        task_start = 0\n        task_end = round(ncs_reader.segment_t_stop(block_index=0, seg_index=task_end_segment_index), 4)\n\n    # I believe this is in number of seconds till start(if theoretically correct), the problem is that the sampling\n    # rate is an average given to us by neuralynx\n    task_start_segment_time = round(ncs_reader.get_signal_t_start(block_index=0,\n                                                                  seg_index=task_start_segment_index), 4) # seconds\n    # to be precise ignore a certain number of samples from task_start_segment_time\n    # we should really only ever be 4 decimal place precise for up to 32K\n    task_start_segment_diff = int(round(task_start - task_start_segment_time, 4) * sampling_rate)  # samples\n\n    # to be precise ignore that last n samples past task_end\n    task_end_segment_time = round(ncs_reader.segment_t_stop(block_index=0, seg_index=task_end_segment_index), 4)\n    task_end_segment_diff = int(round(task_end_segment_time-task_end, 4) * sampling_rate)\n\n    array_size = round(task_end-task_start, 4) * sampling_rate\n    timestamps = np.linspace(task_start, task_end, int(array_size))\n    interp = np.zeros((int(array_size), ))\n    ncs_signal = np.zeros((int(array_size), ))\n    for i in range(task_start_segment_index, task_end_segment_index+1):\n        # First stop. Get the time_segment_start and t_end for each segment.\n        time_segment_start = ncs_reader.get_signal_t_start(block_index=0, seg_index=i)\n        seg_size = ncs_reader.get_signal_size(block_index=0, seg_index=i)  # samples\n        signal_segment = ncs_reader.get_analogsignal_chunk(seg_index=i)\n        if i == task_start_segment_index:\n            total_segment = ncs_reader.rescale_signal_raw_to_float(signal_segment, dtype='float32').T[0]\n            start_index = 0\n            # get the segment size after discounting those starting samples\n            start_seg_size = int(round(seg_size - task_start_segment_diff, 4))\n            ncs_signal[start_index: start_index+start_seg_size] = total_segment[task_start_segment_diff:]\n        elif i == task_end_segment_index:\n            total_segment = ncs_reader.rescale_signal_raw_to_float(signal_segment, dtype='float32').T[0]\n            end_index = len(ncs_signal)\n            # get the segment size after discounting the last samples past task_end\n            end_seg_size = int(round(seg_size - task_end_segment_diff, 4))\n            ncs_signal[end_index-end_seg_size:] = total_segment[:end_seg_size]\n        else:\n            start_index = int(round(time_segment_start - task_start, 4) *\n                              sampling_rate)\n            # rescale to uV\n            ncs_signal[start_index:start_index+seg_size] = ncs_reader.rescale_signal_raw_to_float(signal_segment,\n                                                                                                  dtype='float32').T[0]\n        if i &gt; task_start_segment_index:\n            previous_segment_stop = ncs_reader.segment_t_stop(block_index=0, seg_index=i-1)\n            if abs(time_segment_start-previous_segment_stop) &lt; 1/sampling_rate:\n                continue\n            else:\n                if abs(time_segment_start-previous_segment_stop) &gt; 10:\n                    print('There is at least 10 seconds of missing data see below')\n                    print(previous_segment_stop, time_segment_start)\n                    continue\n                # 01/18/2024 - Consider a version of this script that doesn't interpolate, to allow for better alignment\n                # to spike data where this has in fact not been done (spike sorting (Osort) doesn't care about\n                # timestamping at all)\n                previous_seg_signal = ncs_reader.get_analogsignal_chunk(seg_index=i-1)\n                previous_seg_signal_scaled = ncs_reader.rescale_signal_raw_to_float(previous_seg_signal,\n                                                                                    dtype='float32').T[0]\n                previous_seg_time_start = ncs_reader.get_signal_t_start(block_index=0, seg_index=i-1)\n                previous_seg_size_samples = ncs_reader.get_signal_size(block_index=0, seg_index=i-1)\n                curr_seg_time_end = ncs_reader.segment_t_stop(block_index=0, seg_index=i)\n                current_seg_signal_scaled = ncs_signal[start_index:start_index+seg_size]\n                data_y = np.concatenate((previous_seg_signal_scaled, current_seg_signal_scaled))\n                data_t = np.concatenate((np.linspace(previous_seg_time_start, previous_segment_stop,\n                                                     previous_seg_size_samples), np.linspace(time_segment_start,\n                                                                                             curr_seg_time_end,\n                                                                                             seg_size)))\n\n                # how many missing samples\n                missing_samples_start_ind = previous_seg_size_samples-1\n                missing_samples_end_ind = int((time_segment_start-previous_seg_time_start)*sampling_rate)\n                # print(missing_samples_end)\n                missing_samples = missing_samples_end_ind-missing_samples_start_ind\n                # Define a range around the missing samples\n                range_size = 100\n\n                # Determine the sample indices for interpolation, 100 before and after the missing samples\n                interpolation_start_ind = max(0, missing_samples_start_ind - range_size)\n                interpolation_end_ind = min(len(data_y), missing_samples_end_ind + range_size)\n                # Use only the surrounding data for interpolation\n                local_data_y = data_y[interpolation_start_ind:interpolation_end_ind]\n                local_data_t = data_t[interpolation_start_ind:interpolation_end_ind]\n\n                # Create the interpolated data range over the missing sample range\n                interp_data_t = np.linspace(data_t[missing_samples_start_ind], data_t[missing_samples_start_ind+1],\n                                            missing_samples)\n\n                # Select interpolation method based on the keyword\n                if interp_type == 'cubic':\n                    cs = CubicSpline(local_data_t, local_data_y)\n                    data_x_interp = cs(interp_data_t)\n                elif interp_type == 'linear':\n                    data_x_interp = np.interp(interp_data_t, local_data_t, local_data_y)\n                else:\n                    raise NotImplementedError('Interpolation not found.')\n\n                # data fill in\n                ncs_signal[start_index-missing_samples:start_index] = data_x_interp[:missing_samples]\n                interp[start_index-missing_samples:start_index] = np.ones((missing_samples_end_ind -\n                                                                           missing_samples_start_ind, ))\n    return ncs_signal, sampling_rate, interp, timestamps\n</code></pre>"},{"location":"documentation/manual_process/","title":"Manual Processing tools","text":""},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.reformat_event_labels","title":"<code>reformat_event_labels(subject, session, task, data_directory, annotations_directory, extension=None)</code>","text":"<p>This script takes the events files, reads the timestamps in, and organizes them suitably for the data_viewer. Outputs the events as a csv file :param subject: (str). Subject :param session: (str). Session :param task: (srt). Task the subject completed in this session :param data_directory: (Path). The Path object that points to the neuralynx data directory :param annotations_directory: (Path). The Path object that points to where the annotations file will go. :param extension: str optional. :return:</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def reformat_event_labels(subject, session, task, data_directory, annotations_directory, extension=None):\n    \"\"\"\n    This script takes the events files, reads the timestamps in, and organizes them suitably for\n    the data_viewer. Outputs the events as a csv file\n    :param subject: (str). Subject\n    :param session: (str). Session\n    :param task: (srt). Task the subject completed in this session\n    :param data_directory: (Path). The Path object that points to the neuralynx data directory\n    :param annotations_directory: (Path). The Path object that points to where the annotations file will go.\n    :param extension: str optional.\n    :return:\n    \"\"\"\n    event_times, event_labels, global_start, event_file = get_event_times(data_directory, extension=extension)\n    if len(event_times) == 0:\n        source_epoch = pd.DataFrame(np.array([[], [], []]).T, columns=['time', 'duration', 'label'])\n    else:\n        event_times_sec = event_times*10**-6-global_start\n        durations = np.ones((event_times_sec.shape[0], ))*0.5\n        source_epoch = pd.DataFrame(np.array([event_times_sec, durations, event_labels]).T, columns=['time', 'duration',\n                                                                                                     'label'])\n\n    file_root, _ = os.path.splitext(event_file)\n    annotations_file = f'{subject}_{session}_{task}_{file_root}.csv'\n    if annotations_file in os.listdir(annotations_directory):\n        print('Annotations File exists, so nothing was written. Double check to see if it matches expectation.')\n    else:\n        source_epoch.to_csv(annotations_directory / annotations_file, index=False)\n    return event_file\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.photodiode_check_viewer","title":"<code>photodiode_check_viewer(subject, session, task, data_directory, annotations_directory, diagnostic=False, task_start=0.0, events_filename=None)</code>","text":"<p>This script is a generalized dataviewer to look at a photodiode signal, and bring up the events. With the viewer, we can make annotations and save them to a csv file. Additionally, if the diagnostic optional parameter is set to True, this function will also preprocess the photodiode to check that the signal on TTL is good. :param subject: (string) The patient ID :param session: (string) Session of the experiment. Useful if patient completed more than one session of a task. :param task: (string) Which task :param data_directory: (Path object) Where the data lives :param annotations_directory: (Path object) Where we want to put the annotations of the data :param diagnostic: (bool) (optional) If True we will also plot diagnostics, and preprocess photodiode and overlay it. :param task_start: (float) (optional) The start time of the task :param events_filename: (str) (optional) The extension of the file (in case multiple datasets in the same folder) :return:</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def photodiode_check_viewer(subject, session, task, data_directory, annotations_directory, diagnostic=False,\n                            task_start=0., events_filename=None):\n    \"\"\"\n    This script is a generalized dataviewer to look at a photodiode signal, and bring up the events.\n    With the viewer, we can make annotations and save them to a csv file. Additionally, if the diagnostic optional\n    parameter is set to True, this function will also preprocess the photodiode to check that the signal on TTL is good.\n    :param subject: (string) The patient ID\n    :param session: (string) Session of the experiment. Useful if patient completed more than one session of a task.\n    :param task: (string) Which task\n    :param data_directory: (Path object) Where the data lives\n    :param annotations_directory: (Path object) Where we want to put the annotations of the data\n    :param diagnostic: (bool) (optional) If True we will also plot diagnostics, and preprocess photodiode and overlay\n    it.\n    :param task_start: (float) (optional) The start time of the task\n    :param events_filename: (str) (optional) The extension of the file (in case multiple datasets in the same folder)\n    :return:\n    \"\"\"\n\n    # Underlying assumption is that data is organized as subject/session/raw, this is where the .ncs files live\n    all_files_list = os.listdir(data_directory)\n    # electrode_files = [file_path for file_path in all_files_list if (re.match('m.*ncs', file_path) and not\n    #                file_path.endswith(\".nse\"))]\n\n    ext = events_filename.replace('.nev', '.ncs')\n    ext = ext.replace('events', '')\n    ext = ext.replace('Events', '')\n    if ext is not None and ext != '.ncs':\n        warnings.warn(f\"Most likely multiple photodiode files, picking {ext} now\")\n        ph_files = [file_path for file_path in all_files_list if file_path.endswith(ext) and\n                    (file_path.startswith('photo1') or file_path.startswith('Photo') or file_path.startswith('PH_Diode'))]\n    else:\n        ph_files = [file_path for file_path in all_files_list if file_path.endswith('.ncs') and\n                    (file_path.startswith('photo1') or file_path.startswith('Photo') or file_path.startswith('PH_Diode'))]\n        if len(ph_files) &gt; 1:\n            warnings.warn(\"Multiple photodiode files picking the base one now\")\n            ph_files = [file for file in ph_files if (file.endswith('photo1.ncs') or file.endswith('photo.ncs') or file.startswith('PH_Diode'))]\n    assert len(ph_files) == 1\n    ph_filename = ph_files[0]\n\n    # We'll read in the photodiode signal\n    ph_signal, sampling_rate, interp, timestamps = read_task_ncs(data_directory, ph_filename)\n\n    # we'll make some sanity plots to check photodiode at a glance, and preprocess to double-check event trigger\n    # is good\n    if diagnostic:\n        diagnostic_time_series_plot(ph_signal, sampling_rate, electrode_name='Photodiode')\n        # Use the diagnostic plot to limit the time interval we process photodiode in\n        start_time = input('Type start time (in seconds) if not the start of signal, else press enter: ')\n        end_time = input('Type end time (in seconds) if not the end of signal, else press enter: ')\n        if len(start_time) == 0:\n            start_time = 0\n        else:\n            start_time = int(start_time)\n        if len(end_time) == 0:\n            end_time = int(ph_signal.shape[0])\n        else:\n            end_time = int(end_time)\n\n        ph_signal = ph_signal[int(start_time*sampling_rate):int(sampling_rate * end_time)]\n        # timestamps = timestamps[int(start_time*sampling_rate):int(sampling_rate * end_time)]\n        t_start = start_time\n\n        # next step to this is to add my thresholding for photodiode\n        ph_signal_bin = binarize_ph(ph_signal, sampling_rate)\n        dataset = np.vstack([ph_signal, ph_signal_bin]).T\n        labels = np.array([ph_filename, 'Photodiode Binarized'])\n    else:\n        t_start = task_start\n        dataset = np.expand_dims(ph_signal, axis=1)\n        labels = np.expand_dims(np.array([ph_filename]), axis=1)\n\n    app = mkQApp()\n\n    # Create main window that can contain several viewers\n    win = MainViewer(debug=True, show_auto_scale=True)\n\n    # Create a viewer for signal\n    # T_start essentially rereferences the start time of the dataset, but leaves the annotations alone\n    # be wary of this\n    view1 = TraceViewer.from_numpy(dataset, sampling_rate, t_start, 'Photodiode', channel_names=labels)\n\n    # TO DO\n    # Figure out a better way to scale the different signals when presented\n    # view1.params['scale_mode'] = 'same_for_all'\n    view1.auto_scale()\n    win.add_view(view1)\n\n    # annotation file details\n    possible_labels = [f'{task} duration']\n    file_root, _ = os.path.splitext(events_filename)\n    file_path = annotations_directory / f'{subject}_{session}_{task}_{file_root}.csv'\n    source_epoch = CsvEpochSource(file_path, possible_labels)\n\n    # create a viewer for the encoder itself\n    view2 = EpochEncoder(source=source_epoch, name='Tagging events')\n    win.add_view(view2)\n\n    # show main window and run Qapp\n    win.show()\n    app.exec()\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.photodiode_event_check_viewer","title":"<code>photodiode_event_check_viewer(subject, session, task, dataset, labels, sampling_rate, annotations_directory)</code>","text":"<p>This code will plot the ph_signal, the binarized signal and the computed event onsets and offsets with the goal of getting the types of events properly annotated. :param dataset: (np.array) :parma labels: (np.array) - label associated with dataset :param sampling_rate: (float) :param event_onsets: (np.array) :param event_offsets: (np.array) :param annotations_directory: (Path object) :return: .csv file</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def photodiode_event_check_viewer(subject, session, task, dataset, labels, sampling_rate, annotations_directory):\n    \"\"\"\n    This code will plot the ph_signal, the binarized signal and the computed event onsets and offsets\n    with the goal of getting the types of events properly annotated.\n    :param dataset: (np.array)\n    :parma labels: (np.array) - label associated with dataset\n    :param sampling_rate: (float)\n    :param event_onsets: (np.array)\n    :param event_offsets: (np.array)\n    :param annotations_directory: (Path object)\n    :return: .csv file\n    \"\"\"\n    t_start = 0 # change and make an argument if needed\n    app = mkQApp()\n\n    # Create main window that can contain several viewers\n    win = MainViewer(debug=True, show_auto_scale=True)\n\n    # Create a viewer for signal\n    # T_start essentially rereferences the start time of the dataset, but leaves the annotations alone\n    # be wary of this\n    view1 = TraceViewer.from_numpy(dataset, sampling_rate, t_start, 'Photodiode Signals', channel_names=labels)\n\n    # TO DO\n    # Figure out a better way to scale the different signals when presented\n    # view1.params['scale_mode'] = 'same_for_all'\n    view1.auto_scale()\n    win.add_view(view1)\n\n    # annotation file details\n    possible_labels = ['block start', 'trial start', 'feedback', 'event']\n    events_filename = annotations_directory / f'{subject}_{session}_{task}_ph_events.csv'\n\n    source_epoch = CsvEpochSource(events_filename, possible_labels)\n\n    # create a viewer for the encoder itself\n    view2 = EpochEncoder(source=source_epoch, name='Tagging events')\n    win.add_view(view2)\n\n    # show main window and run Qapp\n    win.show()\n    app.exec()\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.data_clean_viewer","title":"<code>data_clean_viewer(subject, session, task, annotations_directory, electrode_names, dataset, fs)</code>","text":"<p>This function serves to look at lfp signals and look at which is the reference or to look at the macrowires and clean the data for epileptic activity Current: Only array functionality, so data should be hard loaded as an array for this function to work :param subject: (string) subject id :param session: (string) session id :param task: (string) task id :param annotations_directory: (Path) path object :param electrode_names: (1d array) :param dataset: (n_electrodes, n_timepoints) :param fs: (int) :return:</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def data_clean_viewer(subject, session, task, annotations_directory, electrode_names, dataset, fs):\n    \"\"\"\n    This function serves to look at lfp signals and look at which is the reference or to look at the\n    macrowires and clean the data for epileptic activity\n    Current: Only array functionality, so data should be hard loaded as an array for this function to work\n    :param subject: (string) subject id\n    :param session: (string) session id\n    :param task: (string) task id\n    :param annotations_directory: (Path) path object\n    :param electrode_names: (1d array)\n    :param dataset: (n_electrodes, n_timepoints)\n    :param fs: (int)\n    :return:\n    \"\"\"\n    possible_labels = ['bad epileptic activity macro', 'bad eight hertz noise', 'bad epileptic activity micro',\n                       'bad epileptic activity both', 'reference electrode', 'microPED electrode',\n                       'white noise electrode', 'clipping noise electrode', 'epileptic macrocontact',\n                       'white noise macrocontact']\n    file_path = annotations_directory / f'{subject}_{session}_{task}_post_timestamping_events.csv'\n    source_epoch = CsvEpochSource(file_path, possible_labels)\n\n    t_start = 0.\n    # you must first create a main Qt application (for event loop)\n    app = mkQApp()\n\n    # Create the main window that can contain several viewers\n    win = MainViewer(debug=True, show_auto_scale=True)\n\n    # create a viewer for signal\n    view1 = TraceViewer.from_numpy(dataset.T, fs, t_start, 'Microwires', channel_names=electrode_names)\n    # view1 = TraceViewer.from_neo_analogsignal(analog_signals, 'sigs')\n    view1.params['scale_mode'] = 'same_for_all'\n    view1.auto_scale()\n    win.add_view(view1)\n\n    # create a viewer for the encoder itself\n    view2 = EpochEncoder(source=source_epoch, name='Tagging events')\n    win.add_view(view2)\n\n    #\n    # view3 = EventList(source=source_epoch, name='events')\n    # win.add_view(view3)\n    # show main window and run Qapp\n    win.show()\n\n    app.exec()\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.write_timestamps","title":"<code>write_timestamps(subject, session, task, event_folder, annotations_directory, local_data_directory, events_filename)</code>","text":"<p>Looks in event folders for labels. Takse labels and converts them to machine time and places into a .txt file useful for spike sorting. :param subject: (string) Subject identifier :param session: (string) Session identifier (1 if subject only completed one run of the experiment. :param task: (string) Task identifier. The task or experiment subject completed. :param annotations_directory: This is the folder where manual annotations are found :param event_folder: This is the folder where events live (helpful to get global machine time) :param local_data_directory:  This is where the microwire data to be sorted is :param events_filename: If multiple events exist :return: None. A txt file is generated with relative timestamps if needed, or not if not needed.</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def write_timestamps(subject, session, task, event_folder, annotations_directory, local_data_directory,\n                     events_filename):\n    \"\"\"\n    Looks in event folders for labels. Takse labels and converts them to machine time and places into a .txt file useful\n    for spike sorting.\n    :param subject: (string) Subject identifier\n    :param session: (string) Session identifier (1 if subject only completed one run of the experiment.\n    :param task: (string) Task identifier. The task or experiment subject completed.\n    :param annotations_directory: This is the folder where manual annotations are found\n    :param event_folder: This is the folder where events live (helpful to get global machine time)\n    :param local_data_directory:  This is where the microwire data to be sorted is\n    :param events_filename: If multiple events exist\n    :return: None. A txt file is generated with relative timestamps if needed, or not if not needed.\n    \"\"\"\n    file_root, _ = os.path.splitext(events_filename)\n    labels_file = pd.read_csv(annotations_directory / f'{subject}_{session}_{task}_{file_root}.csv')\n    task_label = labels_file[labels_file.label == f\"{task} duration\"]\n    if len(task_label) == 0:\n        raise ValueError('No label found for task. Please re-run script to find task relevant data.')\n    start_time_sec = task_label['time'].iloc[0].astype(float)\n    end_time_sec = start_time_sec + task_label['duration'].iloc[0].astype(float)\n\n    ext = events_filename[-6:]\n    _, _, global_start, _ = get_event_times(event_folder, extension=ext)\n    # microsec_sec = 10**-6\n    sec_microsec = 10**6\n    start_time_machine = (start_time_sec + global_start) * sec_microsec\n    end_time_machine = (end_time_sec + global_start) * sec_microsec\n    timestamps_file = local_data_directory / f\"timestampsInclude.txt\"\n    specified_file = annotations_directory / f\"which_files.txt\"\n    # generate directory if it doesn't exist\n    if not os.path.exists(local_data_directory):\n        os.mkdir(local_data_directory)\n    with open(timestamps_file, 'w+') as f:\n        f.write(f'{int(start_time_machine)}    {int(end_time_machine)}')\n    with open(specified_file, 'w+') as f:\n        f.write(f'{file_root}')\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.su_timestamp_process","title":"<code>su_timestamp_process(subject, session, task, data_directory, annotations_directory, results_directory)</code>","text":"<p>Master script that runs basic pipeline to get timestamps file for sorting minimally using OSort Matlab scripts. :param subject: (str). Subject identifier :param session: (str). Session identifier, useful if subject ran more than one session. :param task: (str). Task identifier. The task the subject ran. :param data_directory: (Path). Path object to data where events file and photodiode file lives :param annotations_directory: (Path). Path object that points to where we'd like to store annotations and metadata. :param results_directory: (Path). Path object that points to where the timestampInclude.txt file will end up. Ideally ends up in spike_sorting folder alongside 'raw' folder that contains microwire .ncs files</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def su_timestamp_process(subject, session, task, data_directory, annotations_directory, results_directory):\n    \"\"\"\n    Master script that runs basic pipeline to get timestamps file for sorting minimally using OSort Matlab scripts.\n    :param subject: (str). Subject identifier\n    :param session: (str). Session identifier, useful if subject ran more than one session.\n    :param task: (str). Task identifier. The task the subject ran.\n    :param data_directory: (Path). Path object to data where events file and photodiode file lives\n    :param annotations_directory: (Path). Path object that points to where we'd like to store annotations and metadata.\n    :param results_directory: (Path). Path object that points to where the timestampInclude.txt file will end up. Ideally\n    ends up in spike_sorting folder alongside 'raw' folder that contains microwire .ncs files\n    \"\"\"\n    # double check if there are multiple events files\n    _, _, global_start, event_files = get_event_times(data_directory)\n\n    # separate pipelines for one vs multiple events files\n    if len(event_files) &gt; 1 and type(event_files) == type([]):\n        print('Multiple Events Files, we will go through the separate datasets one at a time')\n        print(event_files)\n        for event_file in event_files:\n            print(f'Loading event file: {event_file}')\n            ext = event_file[-6:]\n            # print(ext)\n            reformat_event_labels(subject, session, task, data_directory, annotations_directory, extension=ext)\n            photodiode_check_viewer(subject, session, task, data_directory, annotations_directory, diagnostic=False,\n                                    events_filename=event_file)\n            file_root, _ = os.path.splitext(event_file)\n            labels_file = pd.read_csv(annotations_directory / f'{subject}_{session}_{task}_{file_root}.csv')\n            task_label = labels_file[labels_file.label == f\"{task} duration\"]\n            if len(task_label) == 0:\n                continue\n            else:\n                target_file = event_file\n                print(f'You have determined that {event_file} has task relevant data')\n                break\n    else:\n        print('One Event File, loading now')\n        reformat_event_labels(subject, session, task, data_directory, annotations_directory)\n        photodiode_check_viewer(subject, session, task, data_directory, annotations_directory, diagnostic=False,\n                                events_filename=event_files)\n        file_root, _ = os.path.splitext(event_files)\n        labels_file = pd.read_csv(annotations_directory / f'{subject}_{session}_{task}_{file_root}.csv')\n        task_label = labels_file[labels_file.label == f\"{task} duration\"]\n        target_file = event_files\n\n    try:\n        print('Writing machine time timestamps for the task to spike_sorting folder')\n        write_timestamps(subject, session, task, data_directory, annotations_directory, results_directory,\n                         events_filename=target_file)\n    except NameError:\n        raise NameError(\"No events file seems to have been made. Maybe you didn't add a task event?\")\n</code></pre>"},{"location":"documentation/manual_process/#intracranial_ephys_utils.manual_process.get_annotated_task_start_time","title":"<code>get_annotated_task_start_time(subject, session, task, annotations_directory)</code>","text":"<p>This function serves as a helper function to grab the start and end times of the task, after annotating the photodiode script. Will only work if annotations file exists, and duration event has been made. Recall that loading in .ncs files relies on loading in many segments of variable length, so we typically load in more data than what start and end time would have you believe :param subject: :param session: :param task: :param annotations_directory: :return: start_time_sec (float) Start time in seconds for the task. Reference is the start of the file recording. :return: end_time_sec (float) End time in seconds for the task. Reference is the start of the file recording. :return: duration (float) Duration in seconds for the task.</p> Source code in <code>intracranial_ephys_utils/manual_process.py</code> <pre><code>def get_annotated_task_start_time(subject, session, task, annotations_directory):\n    \"\"\"\n    This function serves as a helper function to grab the start and end times of the task, after annotating the\n    photodiode script. Will only work if annotations file exists, and duration event has been made.\n    Recall that loading in .ncs files relies on loading in many segments of variable length, so we typically load in\n    more data than what start and end time would have you believe\n    :param subject:\n    :param session:\n    :param task:\n    :param annotations_directory:\n    :return: start_time_sec (float) Start time in seconds for the task. Reference is the start of the file recording.\n    :return: end_time_sec (float) End time in seconds for the task. Reference is the start of the file recording.\n    :return: duration (float) Duration in seconds for the task.\n    \"\"\"\n    labels_file = pd.read_csv(annotations_directory / f'{subject}_{session}_{task}_events.csv')\n    task_label = labels_file[labels_file.label == f\"{task} duration\"]\n    start_time_sec = task_label['time'].iloc[0].astype(float)\n    end_time_sec = start_time_sec + task_label['duration'].iloc[0].astype(float)\n    return start_time_sec, end_time_sec, task_label['duration'].iloc[0].astype(float)\n</code></pre>"},{"location":"documentation/plot_data/","title":"Plotting tools","text":""},{"location":"documentation/plot_data/#intracranial_ephys_utils.plot_data.diagnostic_time_series_plot","title":"<code>diagnostic_time_series_plot(lfp_signal, freq, total_time=None, output_folder=None, electrode_name='', subject=None, session=None)</code>","text":"<p>Plot time series given a sampling rate to check for any weird idiosyncracies in data. Args:     lfp_signal: (numpy array). The time series signal, ideal n_timepoints x 1.     freq: (int). The sampling rate of the signal. To get a good estimate of time     total_time: (float) (optional) Definite positive, used to limit the amount of time visualized.     output_folder: (Path) (optional) Folder where to keep data.     electrode_name: (string) (optional) Name of electrode if saving, or want a title     subject: (string) (optional) Name of subject if saving     session: (string) (optional) Name of session if saving</p> <p>Returns:</p> Source code in <code>intracranial_ephys_utils/plot_data.py</code> <pre><code>def diagnostic_time_series_plot(lfp_signal, freq, total_time=None, output_folder=None, electrode_name='',\n                                subject=None, session=None):\n    \"\"\"\n    Plot time series given a sampling rate to check for any weird idiosyncracies in data.\n    Args:\n        lfp_signal: (numpy array). The time series signal, ideal n_timepoints x 1.\n        freq: (int). The sampling rate of the signal. To get a good estimate of time\n        total_time: (float) (optional) Definite positive, used to limit the amount of time visualized.\n        output_folder: (Path) (optional) Folder where to keep data.\n        electrode_name: (string) (optional) Name of electrode if saving, or want a title\n        subject: (string) (optional) Name of subject if saving\n        session: (string) (optional) Name of session if saving\n\n    Returns:\n\n    \"\"\"\n    fig2, ax = plt.subplots(4, 1)\n\n    ax[0].plot(np.linspace(0, 1, num=int(freq)), lfp_signal[0:int(freq)])\n    ax[0].set_title(f'First second')\n    midlevel_time = 30\n    ax[1].plot(np.linspace(0, midlevel_time, num=int(freq*midlevel_time)), lfp_signal[0:int(midlevel_time*freq)])\n    ax[1].set_title(f'First {midlevel_time} seconds')\n    sec_in_minute = 60\n    ax[2].plot(np.linspace(0, lfp_signal.shape[0]/freq/sec_in_minute, lfp_signal.shape[0]), lfp_signal)\n    ax[2].set_title('Entire task in minutes')\n    ax[3].plot(np.linspace(midlevel_time, 0, num=int(freq*midlevel_time)), lfp_signal[-int(midlevel_time*freq):])\n    ax[3].set_title(f'Last {midlevel_time} seconds')\n\n    for axis in ax:\n        axis.set_ylabel('Voltage (uV)')\n        axis.set_xlabel('Time (s)')\n    ax[2].set_xlabel('Time (m)')\n    if total_time is not None:\n        ax[2].set_xlim([0, total_time])\n    if electrode_name != '':\n        plt.suptitle(f'Time Courses for {electrode_name}')\n    if output_folder is not None:\n        plt.savefig(output_folder / f'{subject}_{session}_{electrode_name}.png')\n        plt.close()\n    else:\n        plt.tight_layout()\n        plt.show()\n    return None\n</code></pre>"},{"location":"documentation/preprocess/","title":"Preprocessing tools","text":""},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.otsu_intraclass_variance","title":"<code>otsu_intraclass_variance(time_series, threshold)</code>","text":"<p>Otsu's intra-class variance. If all datapoints are above or below the threshold, this will throw a warning that can safely be ignored.</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def otsu_intraclass_variance(time_series, threshold):\n    \"\"\"\n    Otsu's intra-class variance.\n    If all datapoints are above or below the threshold, this will throw a warning that can safely be ignored.\n    \"\"\"\n    return np.nansum(\n        [\n            np.mean(cls) * np.var(time_series, where=cls)\n            #   weight   \u00b7  intra-class variance\n            for cls in [time_series &gt;= threshold, time_series &lt; threshold]\n        ]\n    )\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.otsu_threshold","title":"<code>otsu_threshold(time_series)</code>","text":"<p>Otsu thresholding. I know it's for an image, but it should get the job done here in this time series signal, since it quite literally is two classes with noise. What difference does it make it if the variation in foreground and background happen in time than space. :param time_series: :return:</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def otsu_threshold(time_series):\n    \"\"\"\n    Otsu thresholding. I know it's for an image, but it should get the job done here in this time series signal, since\n    it quite literally is two classes with noise. What difference does it make it if the variation in foreground and\n    background happen in time than space.\n    :param time_series:\n    :return:\n    \"\"\"\n    change = np.min(np.abs(np.diff(time_series))[np.nonzero(np.diff(time_series))])\n    otsu_threshold = min(np.linspace(np.min(time_series)+5*change,\n                                     np.max(time_series)-5*change,100),\n        key=lambda th: otsu_intraclass_variance(time_series, th),\n    )\n    return otsu_threshold\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.decay_step_model","title":"<code>decay_step_model(t, t0, initial, ph_inf, tau)</code>","text":"<p>Model a step followed by exponential decay t0: time of step amplitude: size of step baseline: long_run behavior tau: decay time constant</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def decay_step_model(t, t0, initial, ph_inf, tau):\n    \"\"\"\n    Model a step followed by exponential decay\n    t0: time of step\n    amplitude: size of step\n    baseline: long_run behavior\n    tau: decay time constant\n    \"\"\"\n    y = np.zeros_like(t, dtype=float)\n    mask = t &gt;= t0\n    y[mask] = ph_inf - (ph_inf-initial) * np.exp(-(t[mask] - t0) / tau)\n    y[~mask] = initial\n    return y\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.binarize_ph","title":"<code>binarize_ph(ph_signal, sampling_rate, task_time=None, event_threshold=2, debug=True)</code>","text":"<p>Binarizes the photodiode signal using the midpoint of the signal. Use local midpoints if given a tau. New version of this script uses a high pass filter and then peaks to find timepoints at which the signal changes by a lot and fast. We then try to find the exact timepoints when this happens by localizing runs where the high pass filter result is away from 0. We find the change in average signal before and after these timepoints to get a sense of whether it's an event onset or offset. Finally, we assume we'll find some noise so we threshold these onsets and offsets by the point of greatest difference in changes (heuristic for Otsu threshold) :param event_threshold: (float) - tells us how many standard deviations away from the mean to look for events after bandpass filtering for events :param ph_signal: This is the photodiode signal itself (array of floats) :param sampling_rate: How many samples per second (float) :param task_time: This is how long the task took (in minutes). Helpful to zoom in on particular data regions (float) :return: ph_signal_bin: np.array (same length signal as the input) only now the values should only be 1 and 0. :return: event_onsets_final: Array of event onsets - this gives the indices of each run in ph_signal_bin where a series of 1s will start :return: event_offsets_final np.array - same as above but telling the indices of when each run of 1s ends.</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def binarize_ph(ph_signal, sampling_rate, task_time=None, event_threshold=2, debug=True):\n    \"\"\"\n    Binarizes the photodiode signal using the midpoint of the signal. Use local midpoints if given a tau.\n    New version of this script uses a high pass filter and then peaks to find timepoints at which the signal changes by\n    a lot and fast. We then try to find the exact timepoints when this happens by localizing runs where the high pass\n    filter result is away from 0. We find the change in average signal before and after these timepoints to get\n    a sense of whether it's an event onset or offset. Finally, we assume we'll find some noise so we threshold these\n    onsets and offsets by the point of greatest difference in changes (heuristic for Otsu threshold)\n    :param event_threshold: (float) - tells us how many standard deviations away from the mean to look for events after\n    bandpass filtering for events\n    :param ph_signal: This is the photodiode signal itself (array of floats)\n    :param sampling_rate: How many samples per second (float)\n    :param task_time: This is how long the task took (in minutes). Helpful to zoom in on particular data regions (float)\n    :return: ph_signal_bin: np.array (same length signal as the input) only now the values should only be 1 and 0.\n    :return: event_onsets_final: Array of event onsets - this gives the indices of each run in ph_signal_bin where a series of 1s will start\n    :return: event_offsets_final np.array - same as above but telling the indices of when each run of 1s ends.\n    \"\"\"\n    primary_color = \"mediumblue\"\n    if task_time is not None:\n        total_time = int(task_time*sampling_rate*60)\n    else:\n        total_time = ph_signal.shape[0]\n\n    ph_signal_bin = np.zeros((total_time, ))\n\n    # step 1. quick detrend to remove slow drift\n    detrended_ph = signal.detrend(ph_signal)\n    if debug:\n        plt.hist(detrended_ph)\n        plt.title(f'Detrended Photodiode signal distribution')\n        plt.show()\n\n    detrended_minmaxnorm_ph = (detrended_ph - np.min(detrended_ph)) / (np.max(detrended_ph)- np.min(detrended_ph))\n    if debug:\n        plt.hist(detrended_minmaxnorm_ph)\n        plt.title(f'Detrended and minmax ph signal distribution')\n        plt.show()\n    # step 2: baseline estimation\n    # baseline = np.percentile(detrended_ph, 15)\n\n    # trying out different approaches to filtering\n    # 15 hz for ir95\n    sos = signal.butter(4, 15, 'hp', fs=sampling_rate, output='sos')\n    filtered = signal.sosfiltfilt(sos, detrended_minmaxnorm_ph)\n    stdev = np.std(filtered)\n    # The idea with this method of processing, slightly more complicated than midpoint\n    # is to take points significantly far away from 4 * stdeviations of the high passed signal\n    # Then sequentially take pre and post averages to dictate signs. If sign is positive, then the value after this\n    # is 1, otherwise 0.\n\n    # ph_signal = filtered\n    timepoints = np.arange(ph_signal.shape[0])\n\n    # issue with this method of detecting events is that\n    # with IR95 session 3, had to use 4 std dev\n    # for IR94 session 1, signal is feeble, need to use a different thing\n    events = timepoints[abs(filtered) &gt; event_threshold * stdev]\n    print(len(events))\n    buffer = 0.06*sampling_rate\n    sample_size = int(0.045*sampling_rate)\n    event_breakpoint = 0\n    event_onsets_initial = []\n    event_offsets_initial = []\n\n    sign_changes = []\n    for i, sample_num in enumerate(events):\n        # deal with multiple events and looping\n        if sample_num &lt;= event_breakpoint:\n            continue\n        nearby_occurrences = events[(events &lt; sample_num + buffer) &amp; (events &gt; sample_num - buffer)]\n        num_events = len(nearby_occurrences)\n        if num_events &gt; 1:\n            avg_sample_num = int(np.median(nearby_occurrences))\n            event_breakpoint = np.max(nearby_occurrences) + buffer\n            max_sample_num = int(np.max(nearby_occurrences))\n            min_sample_num = int(np.min(nearby_occurrences))\n            # if max_sample_num - min_sample_num &gt; sample_size:\n            sign_change = (np.average(detrended_minmaxnorm_ph[max_sample_num:min(len(detrended_minmaxnorm_ph)-1,max_sample_num+sample_size)]) -\n                       np.average(detrended_minmaxnorm_ph[max(0, min_sample_num-sample_size):min_sample_num]))\n        else:\n            avg_sample_num = nearby_occurrences[0]\n            sign_change = (np.average(detrended_minmaxnorm_ph[avg_sample_num:min(len(detrended_minmaxnorm_ph)-1, avg_sample_num+sample_size)]) -\n                           np.average(detrended_minmaxnorm_ph[max(0, avg_sample_num-sample_size):avg_sample_num]))\n        # print(sign_change)\n        # plt.plot(ph_signal[avg_sample_num-sample_size:avg_sample_num+sample_size])\n        # plt.show()\n        if sign_change &gt; 0:\n            # positive, event_onset\n            event_onsets_initial.append([avg_sample_num, abs(sign_change)])\n        else:\n            event_offsets_initial.append([avg_sample_num, abs(sign_change)])\n        sign_changes.append(abs(sign_change))\n\n    # find a threshold to use for marking events that depends on the difference in mean signal before or after an event\n    # this depends on having multiple peaks in our resulting distribution, which may not be true, in which case we'll\n    # run into issues\n    # Create a histogram\n    hist, bins = np.histogram(sign_changes)\n\n    sign_change_drop = otsu_threshold(sign_changes)\n    event_onsets = np.array(event_onsets_initial)\n    event_offsets = np.array(event_offsets_initial)\n\n    event_detection_signal = np.zeros(filtered.shape)\n    event_detection_signal[event_onsets[:,0].astype(int)] = 1\n    event_detection_signal[event_offsets[:,0].astype(int)] = -1\n\n    event_onsets = event_onsets[event_onsets[:,1] &gt; sign_change_drop, 0]\n    event_offsets = event_offsets[event_offsets[:,1] &gt; sign_change_drop, 0]\n    if debug:\n        fig, ax = plt.subplots()\n        ax.hist(sign_changes, bins=100, color=primary_color)\n        ax.set_title(f'Histogram of sign changes \\nThreshold {sign_change_drop}')# Add another vertical line at a specific value using plt.vlines\n        # Force matplotlib to calculate the plot layout\n        plt.draw()\n        ymax = ax.get_ylim()[1]\n        ax.vlines(x=sign_change_drop, ymin=0, ymax=ymax*1.2, color='black', linestyle='dashed', linewidth=1, label='Otsu Threshold')\n        ax.legend()\n        plt.ylim([0, ymax])\n        # plt.tight_layout()\n        plt.savefig(f\"{os.pardir}/results/Otsu_thresholding_sample.svg\")\n        plt.show()\n    print('Initial passthrough events')\n    print(len(event_onsets))\n    print(len(event_offsets))\n    if len(event_onsets) &lt; 250 or len(event_offsets) &lt; 250:\n        event_onsets = np.array(event_onsets_initial)\n        event_offsets = np.array(event_offsets_initial)\n        event_onsets = event_onsets[:, 0]\n        event_offsets = event_offsets[:, 0]\n    # okay so we have some events for onsets and offsets, the next step is to make these times more precise.\n    # for each onset, and offset we will fit a radioactive decay with step function to get the precise time that\n    # the transition step started, which should make our estimates much more robust.\n\n    better_event_onsets = []\n    better_event_offsets = []\n    event_window_samples = 0.075 * sampling_rate\n    # for now take 75 msec before and after each event as our window\n    for i, event_onset in enumerate(event_onsets):\n        print(i)\n        start_idx = int(max(0, event_onset - event_window_samples))\n        end_idx = int(min(len(detrended_minmaxnorm_ph), event_onset + event_window_samples))\n        segment_to_fit = detrended_minmaxnorm_ph[start_idx:end_idx]\n        times = (np.arange(len(segment_to_fit)) + start_idx) / sampling_rate\n        # print(times)\n        if i ==0:\n            debug_2=debug\n        else:\n            debug_2=False\n        popt = fitting_ph_response(segment_to_fit, times, debug=debug_2)\n        # we fit the response function, but we only really need the start time\n        better_event_onsets.append(int(popt[0]*sampling_rate))\n\n    for i , event_offset in enumerate(event_offsets):\n        print(i)\n        start_idx = int(max(0, event_offset - event_window_samples))\n        end_idx = int(min(len(detrended_minmaxnorm_ph), event_offset + event_window_samples))\n        segment_to_fit = detrended_minmaxnorm_ph[start_idx:end_idx]\n        times = (np.arange(len(segment_to_fit)) + start_idx) / sampling_rate\n        # print(times)\n        if i ==0:\n            debug_2=debug\n        else:\n            debug_2=False\n        popt = fitting_ph_response(segment_to_fit, times, debug=debug_2)\n        # we fit the response function, but we only really need the start time\n        better_event_offsets.append(int(popt[0]*sampling_rate))\n\n    event_onsets = np.array(better_event_onsets)\n    event_offsets = np.array(better_event_offsets)\n    event_onsets_final = []\n    event_offsets_final = []\n    # Now we have all onsets and offsets, recreate our binarized signals using this\n    # first check that these are the same length, and that the first event_onset is first\n    if (len(event_onsets) == len(event_offsets)) and (event_onsets[0] &lt; event_offsets[0]):\n        print('Events detected are the same size and make sense')\n        print(len(event_onsets))\n    elif len(event_onsets) != len(event_offsets):\n        print('On events and off events do not have the same size')\n    else:\n        print('Issue with start or stop, check manual timestamping')\n    if len(event_onsets) &gt; len(event_offsets):\n        for i, event_onset in enumerate(event_onsets):\n            possible_offsets = event_offsets[event_offsets&gt;event_onset]\n            best_offset = np.min(possible_offsets)\n            ph_signal_bin[int(event_onset): int(best_offset)] = 1.\n            event_onsets_final.append(event_onset)\n            event_offsets_final.append(best_offset)\n    elif len(event_onsets) &lt; len(event_offsets):\n        for i, event_offset in enumerate(event_offsets):\n            possible_onsets = event_onsets[event_onsets&lt;event_offset]\n            best_onset = np.max(possible_onsets)\n            ph_signal_bin[int(best_onset): int(event_offset)] = 1.\n            event_onsets_final.append(best_onset)\n            event_offsets_final.append(event_offset)\n    else:\n        for i, event_onset in enumerate(event_onsets):\n            possible_offsets = event_offsets[event_offsets&gt;event_onset]\n            best_offset = np.min(possible_offsets)\n            ph_signal_bin[int(event_onset): int(best_offset)] = 1.\n            event_onsets_final.append(event_onset)\n            event_offsets_final.append(best_offset)\n\n    event_onsets_final = np.array(event_onsets_final)\n    event_offsets_final = np.array(event_offsets_final)\n\n    if debug:\n        print('huh')\n        # code here to visualize\n\n        app = mkQApp()\n\n        # Create main window that can contain several viewers\n        win = MainViewer(debug=True, show_auto_scale=True)\n\n        dataset = np.vstack((ph_signal, detrended_minmaxnorm_ph, filtered, event_detection_signal,\n                                 event_threshold * stdev*np.ones(filtered.shape), -event_threshold * stdev*np.ones(filtered.shape))).T\n        print(dataset.shape)\n        t_start = 0.\n        labels = ['Raw Photodiode','Min-max + Detrended Photodiode signal', 'Band pass filtered Photodiode signal',\n                  'Events Detected after bubbling procedure', 'std dev', 'std dev 2']\n        # Create a viewer for signal\n        # T_start essentially rereferences the start time of the dataset, but leaves the annotations alone\n        # be wary of this\n        view1 = TraceViewer.from_numpy(dataset, sampling_rate, t_start, 'Photodiode', channel_names=labels)\n\n        # TO DO\n        # Figure out a better way to scale the different signals when presented\n        # view1.params['scale_mode'] = 'same_for_all'\n        view1.auto_scale()\n        win.add_view(view1)\n\n        # show main window and run Qapp\n        win.show()\n        app.exec()\n    print('wait')\n\n    return ph_signal_bin, event_onsets_final, event_offsets_final\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.BCI_LFP_processing","title":"<code>BCI_LFP_processing(lfp_signals, sampling_rate)</code>","text":"<p>This code is meant to resample BCI data down to a numpy array of a more managable sampling, and get rid of power line noise :param lfp_signals: :param sampling_rate: :return:</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def BCI_LFP_processing(lfp_signals, sampling_rate):\n    \"\"\"\n    This code is meant to resample BCI data down to a numpy array of a more managable sampling, and get\n    rid of power line noise\n    :param lfp_signals:\n    :param sampling_rate:\n    :return:\n    \"\"\"\n    # Will take SU 30K Hz down to 1K\n    # Will take iEEG macrocontacts from 2K down to 1K\n    if sampling_rate == 30000:\n        first_factor = 3\n        low_freq = 30\n        high_freq = 3000\n        # bandpass for spikes down to 10K, and then decimate after\n        downsampled_signal = signal.decimate(lfp_signals, first_factor, axis=1)\n        effective_fs = sampling_rate/first_factor\n        butterworth_bandpass = signal.butter(4, (low_freq, high_freq), 'bp', fs=effective_fs, output='sos')\n        bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, downsampled_signal, axis=1)\n        second_factor = 10\n        downsampled_signal_2 = signal.decimate(bandpass_signal, second_factor, axis=1)\n        effective_fs /= second_factor\n    elif sampling_rate == 2000:\n        first_factor = 2\n        low_freq = 0.1\n        high_freq = 900\n        # bandpass for spikes down to 1K, and then decimate after\n        butterworth_bandpass = signal.butter(4, (low_freq, high_freq), 'bp', fs=sampling_rate, output='sos')\n        bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, lfp_signals, axis=1)\n        downsampled_signal_2 = signal.decimate(bandpass_signal, first_factor, axis=1)\n        effective_fs = sampling_rate/first_factor\n    # print(effective_fs)\n    f0 = 60.\n    Q = 30.0  # Quality Factor\n    b_notch, a_notch = signal.iirnotch(f0, Q, effective_fs)\n    processed_signals = signal.filtfilt(b_notch, a_notch, downsampled_signal_2, axis=1)\n    # Get harmonics out of the signal as well, up to 300\n    for i in range(2, 6):\n        b_notch, a_notch = signal.iirnotch(f0*i, Q, effective_fs)\n        processed_signals = signal.filtfilt(b_notch, a_notch, processed_signals, axis=1)\n    return processed_signals, effective_fs\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.broadband_seeg_processing","title":"<code>broadband_seeg_processing(lfp_signals, sampling_rate, lowfreq, high_freq)</code>","text":"<p>This function takes in an lfp signal and performs basic processing. This function is janky but that's only because decimations of too big a factor result in artifacts, so I need one solution for 32K, and a different one for other sampling rates. Preprocessing is as follows, downsample once. We want to make sure we stay above Nyquist limit, so then we run our a 4th order Butterworth to bandpass from lowfreq to highfreq. Downsample once more to get down to 1KHz sampling rate Finally, we'll pass through a Notch filter to get rid of powerline noise and associated harmonics. WARNING: This is NOT good for European data because of the power line noise there(its 50Hz) :param lfp_signals: (np.array) (1, n_samples) :param sampling_rate: (int) :param lowfreq: (int) lower frequency for bandpass :param high_freq: (int) higher frequency for bandpass :return: processed_signals: numpy array shape (1, n_samples) :return: effective_fs: (int) Final sampling rate after processing</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def broadband_seeg_processing(lfp_signals, sampling_rate, lowfreq, high_freq):\n    \"\"\"\n    This function takes in an lfp signal and performs basic processing. This function is janky but that's only\n    because decimations of too big a factor result in artifacts, so I need one solution for 32K, and a different one\n    for other sampling rates.\n    Preprocessing is as follows, downsample once. We want to make sure we stay above Nyquist limit, so then we run our\n    a 4th order Butterworth to bandpass from lowfreq to highfreq. Downsample once more to get down to 1KHz sampling rate\n    Finally, we'll pass through a Notch filter to get rid of powerline noise and associated harmonics.\n    WARNING: This is NOT good for European data because of the power line noise there(its 50Hz)\n    :param lfp_signals: (np.array) (1, n_samples)\n    :param sampling_rate: (int)\n    :param lowfreq: (int) lower frequency for bandpass\n    :param high_freq: (int) higher frequency for bandpass\n    :return: processed_signals: numpy array shape (1, n_samples)\n    :return: effective_fs: (int) Final sampling rate after processing\n    \"\"\"\n    # realizing now that this first line is tricky, think a little bit more about how this will work\n    # with sigurd's data\n    if (sampling_rate &gt;= 32000) and (sampling_rate &lt; 33000):\n        if high_freq == 1000:\n            second_factor = 8\n        elif high_freq == 2000:\n            second_factor = 4\n        else:\n            second_factor = 8\n        first_factor = 4\n        # for microLFP, this brings us down to 8KHz\n        downsampled_signal = signal.decimate(lfp_signals, first_factor)\n        effective_fs = sampling_rate/first_factor\n        butterworth_bandpass = signal.butter(4, (lowfreq, high_freq), 'bp', fs=effective_fs, output='sos')\n        bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, downsampled_signal)\n        # for microLFP, this brings us down to 1/2 Khz\n        downsampled_signal_2 = signal.decimate(bandpass_signal, second_factor)\n        effective_fs /= second_factor\n    elif sampling_rate == 8000:\n        if high_freq == 1000:\n            second_factor = 8\n        elif high_freq == 2000:\n            second_factor = 4\n        else:\n            second_factor = 8\n        # first_factor = 1\n        butterworth_bandpass = signal.butter(4, (lowfreq, high_freq), 'bp', fs=sampling_rate, output='sos')\n        bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, lfp_signals)\n        # for macroLFP, this brings us down to 1 Khz\n        downsampled_signal_2 = signal.decimate(bandpass_signal, second_factor)\n        effective_fs = sampling_rate/second_factor\n    elif sampling_rate == 4000:\n        if high_freq == 1000:\n            second_factor = 4\n        elif high_freq == 2000:\n            second_factor = 2\n        else:\n            raise Exception(f'Figure out better downsampling scheme')\n        # first_factor = 1\n        butterworth_bandpass = signal.butter(4, (lowfreq, high_freq), 'bp', fs=sampling_rate, output='sos')\n        bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, lfp_signals)\n        # for macroLFP, this brings us down to 1 Khz\n        downsampled_signal_2 = signal.decimate(bandpass_signal, second_factor)\n        effective_fs = sampling_rate / second_factor\n    elif sampling_rate == 2000:\n        if high_freq == 1000:\n            second_factor = 2\n            butterworth_bandpass = signal.butter(4, (lowfreq, high_freq), 'bp', fs=sampling_rate, output='sos')\n            bandpass_signal = signal.sosfiltfilt(butterworth_bandpass, lfp_signals)\n            # for macroLFP, this brings us down to 1 Khz\n            downsampled_signal_2 = signal.decimate(bandpass_signal, second_factor)\n            effective_fs = sampling_rate / second_factor\n        elif high_freq == 2000:\n            # don't need to do anything\n            downsampled_signal_2 = lfp_signals\n            effective_fs = sampling_rate\n        else:\n            raise Exception(f'Figure out better downsampling scheme')\n    else:\n        raise Exception(f'Invalid sampling rate {sampling_rate}')\n\n    f0 = 60.\n    q = 30.0  # Quality Factor\n    b_notch, a_notch = signal.iirnotch(f0, q, effective_fs)\n    processed_signals = signal.filtfilt(b_notch, a_notch, downsampled_signal_2)\n\n    # Get harmonics out of the signal as well, up to 300\n    for i in range(2, 6):\n        b_notch, a_notch = signal.iirnotch(f0*i, q, effective_fs)\n        processed_signals = signal.filtfilt(b_notch, a_notch, processed_signals)\n    return processed_signals, int(effective_fs)\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.preprocess_dataset","title":"<code>preprocess_dataset(file_paths, neuro_folder_name, low_pass=1000, task=None, events_file=None)</code>","text":"<p>Read in all data from a given directory and run basic preprocessing on it so I can load it live on my shitty computer. :param file_paths: (list) A list of filenames. Ex(['LAC1.ncs','LAC2.ncs']) :param neuro_folder_name: (Path) The folderpath where the data is held :param task: (optional) A string that dictates the task name, only use if you have the event labels already, so already parsed through the photodiode file and annotated the task duration :param events_file: (optional) (Path) Where the annotation file is located, needed if task is given. :param low_pass: the largest frequency to use for band-pass filtering :return: dataset: Numpy array, shape is (n_channels, n_samples) :return: eff_fs: Effective sampling rate :return: electrode_names: List of electrode names</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def preprocess_dataset(file_paths, neuro_folder_name, low_pass=1000, task=None, events_file=None):\n    \"\"\"\n    Read in all data from a given directory and run basic preprocessing on it so I can load it live on my shitty\n    computer.\n    :param file_paths: (list) A list of filenames. Ex(['LAC1.ncs','LAC2.ncs'])\n    :param neuro_folder_name: (Path) The folderpath where the data is held\n    :param task: (optional) A string that dictates the task name, only use if you have the event labels already, so already parsed\n    through the photodiode file and annotated the task duration\n    :param events_file: (optional) (Path) Where the annotation file is located, needed if task is given.\n    :param low_pass: the largest frequency to use for band-pass filtering\n    :return: dataset: Numpy array, shape is (n_channels, n_samples)\n    :return: eff_fs: Effective sampling rate\n    :return: electrode_names: List of electrode names\n    \"\"\"\n    eff_fs = []\n    electrode_names = []\n    for ind, micro_file_path in enumerate(file_paths):\n        print(micro_file_path)\n        split_tup = os.path.splitext(micro_file_path)\n        ncs_filename = split_tup[0]\n        if ncs_filename.startswith('photo'):\n            lfp_signal, sample_rate, interp, timestamps = read_task_ncs(neuro_folder_name, micro_file_path, task=task,\n                                                                        events_file=events_file)\n            print('timestamps below')\n            print(timestamps)\n            # assume photo is 8K and we're getting down to 1000\n            if low_pass == 1000:\n                first_factor = 8\n            elif low_pass == 2000:\n                first_factor = 4\n            else:\n                first_factor = 8\n            fs = sample_rate / first_factor\n            processed_lfp = signal.decimate(lfp_signal, first_factor)\n            downsampled_timestamps = timestamps[::first_factor]\n            # processed_timestamps = signal.decimate(timestamps, first_factor)\n            print('sliced timestamps below')\n            print(downsampled_timestamps)\n        else:\n            lfp_signal, sample_rate, _, _ = read_task_ncs(neuro_folder_name, micro_file_path, task=task,\n                                                          events_file=events_file)\n            processed_lfp, fs = broadband_seeg_processing(lfp_signal, sample_rate, 0.1, low_pass)\n        if ind == 0:\n            dataset = np.zeros((len(file_paths)+1, processed_lfp.shape[0]))\n            dataset[ind, :] = processed_lfp\n            eff_fs.append(fs)\n            electrode_names.append(ncs_filename)\n            og_file = micro_file_path\n        else:\n            # Currently the loading of photodiode is 3 ms different in size(it's more than the others)\n            if processed_lfp.shape[0] &gt; dataset.shape[1]:\n                print(f'{micro_file_path} array is larger than {og_file}')\n                print(processed_lfp.shape)\n                print(dataset.shape)\n                dataset[ind, 0:dataset.shape[1]] = processed_lfp[0:dataset.shape[1]]\n            else:\n                dataset[ind, :processed_lfp.shape[0]] = processed_lfp[0:processed_lfp.shape[0]]\n            eff_fs.append(fs)\n            electrode_names.append(ncs_filename)\n        if ncs_filename.startswith('photo'):\n            dataset[-1, :] = downsampled_timestamps\n    eff_fs.append(fs)\n    electrode_names.append('Timepoints')\n    return dataset, eff_fs, electrode_names\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.save_small_dataset","title":"<code>save_small_dataset(subject, session, task, events_file, low_pass=1000, data_directory=None)</code>","text":"<p>Load, process, and savedata. :param subject: (string) subject identifier :param session: (string) subject session :param task: (string) task name, used to select only part of entire ncs file, assuming annotations file exists :param events_file: (Path) path to where events annotation file is located :param low_pass: (int) specify low pass frequency, usually :param data_directory: (path) specify where data directory is :return:</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def save_small_dataset(subject, session, task, events_file, low_pass=1000, data_directory=None):\n    \"\"\"\n    Load, process, and savedata.\n    :param subject: (string) subject identifier\n    :param session: (string) subject session\n    :param task: (string) task name, used to select only part of entire ncs file, assuming annotations file exists\n    :param events_file: (Path) path to where events annotation file is located\n    :param low_pass: (int) specify low pass frequency, usually\n    :param data_directory: (path) specify where data directory is\n    :return:\n    \"\"\"\n    if data_directory is None:\n        data_directory = Path(f\"{os.pardir}/data/{subject}/{session}/raw\")\n    results_directory = data_directory.parent.absolute() / \"preprocessed\"\n    print(results_directory)\n    if results_directory.exists():\n        print('Results Directory already Exists')\n    else:\n        os.mkdir(results_directory)\n    all_files_list = os.listdir(data_directory)\n    # electrode_files = [file_path for file_path in all_files_list if (re.match('m.*ncs', file_path) and not\n    #                file_path.endswith(\".nse\"))]\n    electrode_files = [file_path for file_path in all_files_list if file_path.endswith('.ncs')]\n    electrode_files.sort()\n    # electrode_files.append('photo1.ncs')\n    dataset, eff_fs, electrode_names = preprocess_dataset(electrode_files, data_directory, task=task,\n                                                          events_file=events_file, low_pass=low_pass)\n    if len(set(eff_fs)) != 1:\n        warnings.warn('Different effective sampling rates across files')\n    bp = str(int(eff_fs[0]))\n    np.savez(os.path.join(results_directory, f'{subject}_{session}_{task}_lowpass_{bp}'), dataset=dataset,\n             electrode_names=electrode_names, eff_fs=eff_fs)\n    return None\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.save_as_npy","title":"<code>save_as_npy(subject, session, task_name, data_directory, events_file, electrode_selection, one_file=False)</code>","text":"<p>Load data from neuralynx files and package them into .npy files. No preprocessing done to the data, so microwires, and macrocontacts at different sampling rates, treated separately. :param subject: (string) subject identifier :param session: (string) session identifier :param task_name: (string) task identifier :param data_directory: (Path) path object that tells us where the raw data lives (if in the cluster, it won't be in our expected data/subject/session style, hence why this function is the way it is) :param events_file: (Path) path object that tells us where the events file, ideally the events_file contains one event titled f\"{task_name} duration\" :param electrode_selection: (string) Whether to save macrocontact or microwire data :param one_file: (optional) (bool) whether to package data into one file. If false, package data into different files :return:</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def save_as_npy(subject, session, task_name, data_directory, events_file, electrode_selection, one_file=False):\n    \"\"\"\n    Load data from neuralynx files and package them into .npy files. No preprocessing done to the data, so microwires,\n    and macrocontacts at different sampling rates, treated separately.\n    :param subject: (string) subject identifier\n    :param session: (string) session identifier\n    :param task_name: (string) task identifier\n    :param data_directory: (Path) path object that tells us where the raw data lives (if in the cluster, it won't be in\n    our expected data/subject/session style, hence why this function is the way it is)\n    :param events_file: (Path) path object that tells us where the events file, ideally the events_file contains one\n    event titled f\"{task_name} duration\"\n    :param electrode_selection: (string) Whether to save macrocontact or microwire data\n    :param one_file: (optional) (bool) whether to package data into one file. If false, package data into different files\n    :return:\n    \"\"\"\n\n    # Hopefully your file structure is like mine\n    results_directory = Path(f\"{os.pardir}/data/{subject}/{session}/preprocessed\")\n    print(results_directory)\n    if results_directory.exists():\n        print('Results Directory already Exists')\n    else:\n        os.mkdir(results_directory)\n    all_files_list = os.listdir(data_directory)\n    if electrode_selection == \"microwire\":\n        print(all_files_list)\n        electrode_files = [file_path for file_path in all_files_list if file_path.endswith('.ncs')\n                           and file_path.startswith('m') and not file_path.startswith('mic1')]\n        electrode_files.sort()\n        eff_fs = []\n        electrode_names = []\n        print(electrode_files)\n        for ind, micro_file_path in enumerate(electrode_files):\n            print(micro_file_path)\n            split_tup = os.path.splitext(micro_file_path)\n            ncs_filename = split_tup[0]\n            lfp_signal, sample_rate, interp, timestamps = read_task_ncs(data_directory, micro_file_path,\n                                                                        task=task_name,\n                                                                        events_file=events_file)\n            if one_file:\n                if ind == 0:\n                    dataset = np.zeros((len(electrode_files) + 1, lfp_signal.shape[0]))\n                    dataset[ind, :] = lfp_signal\n                    eff_fs.append(sample_rate)\n                    electrode_names.append(ncs_filename)\n                else:\n                    dataset[ind, :] = lfp_signal\n                    eff_fs.append(sample_rate)\n                    electrode_names.append(ncs_filename)\n            else:\n                dataset = lfp_signal\n                bp = str(int(sample_rate))\n                np.savez(os.path.join(results_directory, f'{subject}_{session}_{task_name}_{ncs_filename}_{bp}'),\n                         dataset=dataset, electrode_name=ncs_filename, fs=sample_rate, timestamps=timestamps)\n    elif electrode_selection == \"photodiode\":\n        electrode_files = [file_path for file_path in all_files_list if file_path.endswith('.ncs')\n                           and file_path.startswith('photo') or file_path.startswith('Photo')]\n        electrode_files.sort()\n        eff_fs = []\n        electrode_names = []\n        print(electrode_files)\n        for ind, micro_file_path in enumerate(electrode_files):\n            print(micro_file_path)\n            split_tup = os.path.splitext(micro_file_path)\n            ncs_filename = split_tup[0]\n            lfp_signal, sample_rate, interp, timestamps = read_task_ncs(data_directory, micro_file_path,\n                                                                        task=task_name,\n                                                                        events_file=events_file)\n            dataset = lfp_signal\n            bp = str(int(sample_rate))\n            np.savez(os.path.join(results_directory, f'{subject}_{session}_{task_name}_{ncs_filename}_{bp}'),\n                dataset=dataset, electrode_name=ncs_filename, fs=sample_rate, timestamps=timestamps)\n            # for debug purposes, also save a version of this file at 32KHz\n            og_len = len(lfp_signal) / sample_rate\n            (lfp_signal_upsampled, timestamps_upsampled) = resample(lfp_signal, int(og_len*32000), timestamps)\n            dataset_upsampled = lfp_signal_upsampled\n            bp_up = 32000\n            np.savez(os.path.join(results_directory, f'{subject}_{session}_{task_name}_{ncs_filename}_{bp_up}'),\n                dataset=dataset_upsampled, electrode_name=ncs_filename, fs=bp_up, timestamps=timestamps_upsampled)\n    elif electrode_selection == \"macrocontact\":\n        raise NotImplementedError\n        ########### TO DO\n        # the function will be the same, but just don't know how to do the electrode selection (maybe use lazy reader\n        # to exclude files with a certain sample rate?\n    if one_file:\n        bp = str(int(eff_fs[0]))\n        np.savez(os.path.join(results_directory, f'{subject}_{session}_{task_name}_{electrode_selection}_{bp}'),\n                 dataset=dataset, electrode_names=electrode_names, eff_fs=eff_fs, timestamps=timestamps)\n    return None\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.make_trialwise_data","title":"<code>make_trialwise_data(event_times, electrode_names, fs, dataset, tmin=-1.0, tmax=1.0, baseline=None, annotations=None)</code>","text":"<p>This function serves to convert a dataset that is from start to stop, into one that is organized by trials. :param event_times: (timestamps for trial onsets, offsets, or anything of interest) :param electrode_names: (list) list of strings that contain the name for each signal :param fs: (int) sampling rate :param dataset: (np.array) raw data :param tmin: (opt) :param tmax: (opt) :param baseline: (opt) Tuple that defines the period to use as baseline :param annotations: mne annotations object :return: epochs_object</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def make_trialwise_data(event_times, electrode_names, fs, dataset, tmin=-1., tmax=1., baseline=None, annotations=None):\n    \"\"\"\n    This function serves to convert a dataset that is from start to stop, into one that is organized by trials.\n    :param event_times: (timestamps for trial onsets, offsets, or anything of interest)\n    :param electrode_names: (list) list of strings that contain the name for each signal\n    :param fs: (int) sampling rate\n    :param dataset: (np.array) raw data\n    :param tmin: (opt)\n    :param tmax: (opt)\n    :param baseline: (opt) Tuple that defines the period to use as baseline\n    :param annotations: mne annotations object\n    :return: epochs_object\n    \"\"\"\n\n    # The safest way to do this is to build a mne object, first step is to create the info for that object\n    events = np.zeros((event_times.shape[0], 3))\n    events[:, 0] = event_times\n    events[:, 2] = np.ones((event_times.shape[0],))[:] #rule_codes commented out for now because i need something to run\n    events = events.astype(int)\n    mne_info = mne.create_info(electrode_names, fs, ch_types='seeg')\n    raw_data = mne.io.RawArray(dataset, mne_info)\n    if annotations is not None:\n        raw_data.set_annotations(annotations)\n\n    num_electrodes, num_samples = dataset.shape\n    # Trying to compare some things\n    epochs_object = mne.Epochs(raw_data, events, tmax=tmax, tmin=tmin, baseline=baseline,\n                               reject_by_annotation=False)\n    trial_based_data = epochs_object.get_data(copy=True)\n    epochs_object = mne.Epochs(raw_data, events, tmax=tmax, tmin=tmin, baseline=baseline,\n                               reject_by_annotation=True)\n    return epochs_object, trial_based_data\n</code></pre>"},{"location":"documentation/preprocess/#intracranial_ephys_utils.preprocess.smooth_data","title":"<code>smooth_data(data, fs, window, step)</code>","text":"<p>Smooth data by taking the average in windows, and stepping by some amount of time Expects data to be 3D (number of trials X number of electrodes X number of timepoints) We will smooth by taking the centered average about a window, so the smoothed data will be smaller than expected :param data: np.array :param fs: (int) sampling rate :param window: (float) in seconds, how much to average over, the larger this is the more our signal is smeared. :param step: (float) in seconds. How much to step forward, determines new_fs :return: smoothed_data, new_fs</p> Source code in <code>intracranial_ephys_utils/preprocess.py</code> <pre><code>def smooth_data(data, fs, window, step):\n    \"\"\"\n    Smooth data by taking the average in windows, and stepping by some amount of time\n    Expects data to be 3D (number of trials X number of electrodes X number of timepoints)\n    We will smooth by taking the centered average about a window, so the smoothed data will be smaller than expected\n    :param data: np.array\n    :param fs: (int) sampling rate\n    :param window: (float) in seconds, how much to average over, the larger this is the more our signal is smeared.\n    :param step: (float) in seconds. How much to step forward, determines new_fs\n    :return: smoothed_data, new_fs\n    \"\"\"\n    # first create array that is the processed shape\n    num_epochs, num_electrodes, num_timepoints = data.shape\n    # why should our smoothed data be this?\n    # we'd like to smooth data by taking averages with a window size and moving by a certain step\n    # ideally window is centered, effectively meaning that we can only take as many timepoints that equal to\n    # (num_timepoints/fs - step) / step and this simplifies to below\n    smoothed_data = np.zeros((num_epochs, num_electrodes, int(num_timepoints/(fs*step)-1)))\n    for i in range(smoothed_data.shape[2]):\n        # print(i)\n        if i == 0:\n            smoothed_data[:, :, i] = np.mean(data[:, :, i:int((i+1)*window*fs)], axis=2)\n        elif (i*step*fs + window*fs) &gt; num_timepoints:\n            print('There is an issue with this code and the estimation of the smoothed data size')\n            start = i*step*fs - window/2\n            smoothed_data[:, :, i] = np.mean(data[:, :, int(start): num_timepoints], axis=2)\n        else:\n            # print('int')\n            # print(data.shape)\n            # print(i * step * fs)\n            start = i * step * fs\n            smoothed_data[:, :, i] = np.mean(data[:, :, int(start-window/2*fs): int(start + window/2*fs)], axis=2)\n    new_fs = 1 / step\n    return smoothed_data, new_fs\n</code></pre>"}]}